var searchIndex = JSON.parse('{\
"basic_lexer":{"doc":"basic_lexer is a basic lexical scanner designed for use …","t":[13,3,13,13,13,13,3,13,13,4,13,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11],"n":["Alphanum","File_tokenizer","Float","Integer","Keyword","Newline","Str_tokenizer","Stringlit","Symbol","Token","Verbatim","add_keywords","add_singletons","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","clone","clone_into","column_number","eq","fmt","from","from","from","into","into","into","into_iter","into_iter","line_number","ne","new","new","next","next","no_line_comment","no_line_comment","reset","rest","set_comments","set_keep_comments","set_keep_newline","set_line_comment","set_line_comment","to_owned","try_from","try_from","try_from","try_into","try_into","try_into","type_id","type_id","type_id"],"q":["basic_lexer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""],"d":["tokens that must start with a alphabetical character or …","a Token Iterator on a given file","non-negative base-10 floating-point numbers","non-negative base-10 integers. Negative signs are emitted …","special keywords such as “if”, “else”, “while”…","indicates that a new line has been read; only produced by …","A Token Iterator on a given &str.","string literals, which can span multiple lines if …","non-alphanumeric symbols such as “==”.  Note that a …","Tokens are returned by the iterators Str_tokenizer and …","Verbatim, non-tokenized text such as comments, only …","adds keywords that are to be distinguished from other …","adds characters to be recognized as single-character …","","","","","","","","","returns the current column (character position) on the …","","","","","","","","","","","returns the current line number being read","","","creates a File_tokenizer given a file path, panics if …","","","disables recognition of the line-comment character","disables the recogniton of single-line comments.  This …","resets the tokenizer to recognize a new str.  The next …","returns the untokenized remainder of the str","sets the symbols used to delineate possibly multiple-line …","sets option to keep comments delineated by set_comments …","sets option to emit the Newline token when a new line is …","sets the character to designed a comment.  The rest of …","sets the symbol used to designate a single-line comment.  …","","","","","","","","","",""],"i":[1,0,1,1,1,1,0,1,1,0,1,2,2,3,2,1,3,2,1,1,1,2,1,1,3,2,1,3,2,1,3,2,2,1,3,2,3,2,3,2,3,3,2,2,2,3,2,1,3,2,1,3,2,1,3,2,1],"f":[null,null,null,null,null,null,null,null,null,null,null,[[["str",15]]],[[["str",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["token",4]],[[]],[[],["usize",15]],[[["token",4]],["bool",15]],[[["formatter",3]],["result",6]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["usize",15]],[[["token",4]],["bool",15]],[[["str",15]],["str_tokenizer",3]],[[["str",15]],["file_tokenizer",3]],[[],[["option",4,["token"]],["token",4]]],[[],[["option",4,["token"]],["token",4]]],[[]],[[]],[[["str",15]]],[[],["str",15]],[[["str",15]]],[[["bool",15]]],[[["bool",15]]],[[["char",15]]],[[["str",15]]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]]],"p":[[4,"Token"],[3,"File_tokenizer"],[3,"Str_tokenizer"]]}\
}');
if (window.initSearch) {window.initSearch(searchIndex)};